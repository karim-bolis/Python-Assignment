{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "#from sklearn.linear_model import LogisticRegression     # Logistic Regression\n",
    "from sklearn.neighbors import KNeighborsClassifier      # k-Nearest Neighbours\n",
    "from sklearn.preprocessing import LabelEncoder          # encooding variables\n",
    "from sklearn.preprocessing import StandardScaler        # encooding variables\n",
    "from sklearn.model_selection import train_test_split    # testing our models\n",
    "#from sklearn.preprocessing import OneHotEncoder         # nominal variable\n",
    "#from sklearn.metrics import confusion_matrix            # scoring\n",
    "#from sklearn.tree import DecisionTreeClassifier         # decision trees\n",
    "#from sklearn.tree import DecisionTreeRegressor          # decision trees\n",
    "#from sklearn import tree                                # decision trees\n",
    "from sklearn.decomposition import PCA                   # PCA \n",
    "from sklearn.cluster import KMeans                      # KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27448/2946516264.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load data into a dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcvss_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/cvss/CVSS_data_complete.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# drop unneeded columns that are not useful for analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcvss_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcvss_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cve_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'assigner'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cwe_ids'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'refs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ref_names'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ref_sources'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ref_tags'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'v3_baseScore'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'v3_exploitabilityScore'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'v3_impactScore'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'v3_baseSeverity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# load data into a dataframe\n",
    "cvss_data = pd.read_csv('./data/cvss/CVSS_data_complete.csv')\n",
    "\n",
    "# drop unneeded columns that are not useful for analysis\n",
    "cvss_data = cvss_data.drop(columns=['cve_id','assigner','description','cwe_ids','refs','ref_names','ref_sources','ref_tags','v3_baseScore','v3_exploitabilityScore','v3_impactScore','v3_baseSeverity'])\n",
    "\n",
    "# Checking for empty values\n",
    "print(cvss_data.isnull().sum())\n",
    "\n",
    "# check that columns don't have unusual values\n",
    "for col in cvss_data.columns:\n",
    "    print(col, \":\", cvss_data[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode all columns\n",
    "le = LabelEncoder()\n",
    "for col in cvss_data.columns:\n",
    "    cvss_data[col] = le.fit_transform(cvss_data[col])\n",
    "    \n",
    "    \n",
    "# decompose\n",
    "pca = PCA(n_components=2)\n",
    "cvss_reduced = pca.fit_transform(cvss_data)\n",
    "\n",
    "# cluster\n",
    "kmc = KMeans(n_clusters=6, random_state=0)\n",
    "kmc_model = kmc.fit(cvss_reduced)\n",
    "\n",
    "\n",
    "# plotting\n",
    "colors=[\"red\",\"blue\",\"green\",\"purple\",\"orange\",\"brown\",\"black\"]\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(np.max(kmc_model.labels_)+1):\n",
    "    plt.scatter(cvss_reduced[kmc_model.labels_==i][:,0], cvss_reduced[kmc_model.labels_==i][:,1], label=i, c=colors[i], alpha=0.5)\n",
    "plt.scatter(kmc_model.cluster_centers_[:,0], kmc_model.cluster_centers_[:,1], label='Cluster Centers', c=\"black\", s=200)\n",
    "plt.title(\"K-Means Clustering of CVSS Data\",size=20)\n",
    "plt.xlabel(\"Principle Component 1\", size=16)\n",
    "plt.ylabel(\"Principle Component 2\", size=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started by using a label encoder to assign numbers to all the columns, since all the remanining columns were described by words. I then reduced the data down to 2 dimensions to allow myself to look at the data from the best angle and easily decide the number of clusters needed.\n",
    "I then used K Means clustering to cluster the data. I first tried using 5 clusters as my hyperparameter, but found that this leaves the bottom left data in the graph being divided between the clusters on its right and left, which are quite far away fropm it, so I chose 6 clusters. Using 7 clusters unnecessarily divided up the area on the bottom right of the graph into 3 clusters, which I did not think was accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe containing the data points of each cluster\n",
    "k = np.max(kmc_model.labels_)+1\n",
    "df_clusters = [cvss_data[kmc_model.labels_==i] for i in range(k)]\n",
    "\n",
    "# reset the cvss_data dataframe to include all columns\n",
    "cvss_data = pd.read_csv('./data/cvss/CVSS_data_complete.csv')\n",
    "\n",
    "# print the mean base score for each cluster\n",
    "inc = 1\n",
    "for i in range(np.max(kmc_model.labels_)+1):\n",
    "    mean = np.mean(cvss_data[kmc_model.labels_==i]['v3_baseScore'])\n",
    "    print(\"cluster number\", inc, \"has mean Base Score\", np.asarray(mean), \"\\n\")\n",
    "    inc = inc + 1\n",
    "\n",
    "le = LabelEncoder()\n",
    "cvss_data['v3_baseSeverity'] = le.fit_transform(cvss_data['v3_baseSeverity'])    \n",
    "    \n",
    "    \n",
    "k = np.max(kmc_model.labels_)+1\n",
    "df_clusters = [cvss_data[kmc_model.labels_==i] for i in range(k)]\n",
    "\n",
    "stat_dict = { \n",
    "    'Cluster' : map(lambda x:x+1, list(range(k))),\n",
    "    'Size' :    [len(df_clusters[i]) for i in range(k)],\n",
    "    'Mean Exploitability Score' :            [round(df_clusters[i]['v3_exploitabilityScore'].mean(), 2) for i in range(k)],\n",
    "    'Mean Impact Score' :                    [round(df_clusters[i]['v3_impactScore'].mean(), 2) for i in range(k)],\n",
    "    'Mean Base Severity' :                   [round(df_clusters[i]['v3_baseSeverity'].mean(), 2) for i in range(k)],\n",
    "    'Std Dev Base Severity' :                [round(df_clusters[i]['v3_baseSeverity'].std(), 2) for i in range(k)],\n",
    "    'Mean Base Score' :                      [round(df_clusters[i]['v3_baseScore'].mean(), 2) for i in range(k)],\n",
    "    'Std Dev Base Score' :                   [round(df_clusters[i]['v3_baseScore'].std(), 2) for i in range(k)],\n",
    "}\n",
    "df_cluster_stats = pd.DataFrame(stat_dict)\n",
    "df_cluster_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the mean base score for the clusters shows that they are roughly divided into 2 categories, with clusters 1,3 and 5 in one category, all having a low mean base score, and clusters 2,4 and 6 in the other with high values. This trend is further shown in the value for mean base severity, where clusters 1,3 and 5 have high values, while clusters 2,4 and 6 have low ones. The largest differences between clusters 1,3 and 5 however lie in mean impact score, with all 3 having quite different values. Clusters 2,4 and 6 also differ widely on impact score. Cluster 1 is also notable for having quite anomalistic values for some columns, such as mean exploitability score, mean impact score and base severity standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11,7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "to_plot = []\n",
    "for i in range(np.max(kmc_model.labels_)+1):\n",
    "    to_plot.append(cvss_data[kmc_model.labels_==i]['v3_baseScore'])\n",
    "\n",
    "# boxplot\n",
    "plt.boxplot(to_plot)\n",
    "\n",
    "# category labels, overall labels and title\n",
    "plt.xticks([1,2,3,4,5,6],[\"Cluster 1\", \"Cluster 2\", \"Cluster 3\", \"Cluster 4\", \"Cluster 5\", \"Cluster 6\"])\n",
    "ax.set_xlabel(\"Cluster\")\n",
    "ax.set_ylabel(\"Base Score\")\n",
    "ax.set_title(\"Distribution of Base Score throughout the clusters\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly cluster 1 has a huge amount of variation, which is supported by the std dev of 0.57, which is much lower than all the other clusters.\n",
    "\n",
    "As said before, the base score is clearly divided into two groups, so one would assume one group is the \"threat\" one, containing clusters 2,4 and 6 while the other is the less threatening group, containing clusters 1,3 and 5. The difference between these two groups seems significant and noteworthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BITCOINHEIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into a dataframe\n",
    "bitcoin = pd.read_csv('./data/BitcoinHeistDataSample.csv')\n",
    "\n",
    "# drop unneeded columns\n",
    "bitcoin = bitcoin.drop(columns=['address', 'day'])\n",
    "\n",
    "# Checking for empty values\n",
    "print(bitcoin.isnull().sum())\n",
    "\n",
    "# checking that certain columns don't have values that don't make sense\n",
    "wrong_value_check = ['year', 'income']\n",
    "\n",
    "# print the unique options for each column\n",
    "for col in wrong_value_check:\n",
    "    print(col , \"column has unique values\" , np.sort(bitcoin[col].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_years = np.sort(bitcoin['year'].unique())\n",
    "\n",
    "r_counts = []\n",
    "w_counts = []\n",
    "for year in bit_years:\n",
    "    year_rows = bitcoin[bitcoin.loc[:,\"year\"]==year]\n",
    "    r_count = len(year_rows[year_rows.loc[:,\"label\"]!=\"white\"])\n",
    "    r_counts.append(r_count)\n",
    "    w_count = len(year_rows[year_rows.loc[:,\"label\"]==\"white\"])\n",
    "    w_counts.append(w_count)\n",
    "    \n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "colors = cm.plasma(np.arange(2)/2.)\n",
    "width = 0.5\n",
    "\n",
    "      \n",
    "for idx in range(8):\n",
    "    plt.bar((idx*3), r_counts[idx], 0.35, color=colors[0])\n",
    "    plt.bar((idx*3)+0.4, w_counts[idx], 0.35, color=colors[1])\n",
    "\n",
    "\n",
    "# add descriptions for each bar in the graph, as well as indexes and a title\n",
    "plt.xticks([0.2,3.2,6.2,9.2,12.2,15.2,18.2, 21.2], [\"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\"])\n",
    "ax.set_xlabel(\"Year\", fontsize=15)\n",
    "ax.set_ylabel(\"Volume\", fontsize=15)\n",
    "ax.set_title(\"Comparison of the Volume of White and Ransom records throughout the years\", fontsize=20)\n",
    "ax.legend(('ransom', 'white'), fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and split data with 64% for training, 20% for testing and 16% for validation\n",
    "def data_scale_split(data):\n",
    "    \n",
    "    X = data[['year', 'length', 'weight', 'count', 'looped', 'neighbors', 'income']]\n",
    "    y = data['label']\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(X,y,test_size = 0.2,random_state=2420)\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    ss_model = ss.fit(train_x)\n",
    "    train_x_scaled = ss_model.transform(train_x)\n",
    "    test_x_scaled = ss_model.transform(test_x)    \n",
    "    \n",
    "    train_x_scaled, val_x_scaled, train_y, val_y = train_test_split(train_x_scaled,train_y,test_size = 0.2)\n",
    "    return train_x_scaled, train_y, test_x_scaled, test_y, val_x_scaled, val_y\n",
    "\n",
    "train_x_scaled, train_y, test_x_scaled, test_y, val_x_scaled, val_y = data_scale_split(bitcoin)\n",
    "\n",
    "# test what the best parameter for k is\n",
    "best_k = -1\n",
    "best_score = -1\n",
    "for k in [3,5,7,9,11,13,15]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)    # just change the n_neighbors parameter\n",
    "    \n",
    "    knn_model = knn.fit(train_x_scaled, train_y) # scaled X, un-scaled y\n",
    "    \n",
    "    train_score = knn.score(train_x_scaled, train_y)\n",
    "    \n",
    "    val_score = knn.score(val_x_scaled, val_y)\n",
    "    \n",
    "    print(k, \"Training Score:\", train_score, \"Validation Score: \", val_score)\n",
    "    \n",
    "    # find the best k\n",
    "    if best_score <= val_score:\n",
    "        best_score = val_score\n",
    "        best_model = knn_model\n",
    "        besk_k = k\n",
    "\n",
    "print(f'The best k is {besk_k} and the best val score is {best_score:.4f}')\n",
    "\n",
    "\n",
    "# evalate the best model using the test set\n",
    "print(\"Best Model Test Score: {:.4f}\".format(best_model.score(test_x_scaled, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prepared the data by splitting it into training, test and validation. I then used a wide range of values to test what the best value for k is, to use it as a parameter using the validation set\n",
    "\n",
    "hyperparameters:\n",
    "k=9 as justified above\n",
    "training = 64, test = 20, val = 16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
